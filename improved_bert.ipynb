{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c219c3dd",
   "metadata": {},
   "source": [
    "# For colab environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07ada328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk transformers==4.35.0 torch==2.6.0 torchvision==0.21.0 datasets accelerate==0.24.0 huggingface==0.0.1 datasets==2.14.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26401123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "2.6.0+cu124\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "print(torch.cuda.is_available())\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fc2925e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/BernardMoy/NLP-PCL-Classification.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3efe8fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd NLP-PCL-Classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5eec618c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Mar  1 13:05:37 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.163.01             Driver Version: 550.163.01     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA TITAN Xp                Off |   00000000:02:00.0  On |                  N/A |\n",
      "| 24%   38C    P8             18W /  250W |     506MiB /  12288MiB |     30%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A   2306585      G   /usr/lib/xorg/Xorg                            110MiB |\n",
      "|    0   N/A  N/A   3138881      G   /usr/bin/kalendarac                             3MiB |\n",
      "|    0   N/A  N/A   3147397      G   /usr/lib/firefox/firefox                      173MiB |\n",
      "|    0   N/A  N/A   3199839      G   /usr/share/code/code                          211MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bb33de",
   "metadata": {},
   "source": [
    "# Load train and validation data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb2a830f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from matplotlib import pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "df = pd.read_csv('data/dontpatronizeme_pcl.tsv', sep='\\t')\n",
    "\n",
    "# Remove rows with NA labels \n",
    "df = df.dropna() \n",
    "\n",
    "# Add a bool_labels column for binary classification\n",
    "df[\"bool_labels\"] = df[\"label\"] > 1   # is PCL if >1\n",
    "\n",
    "# train val split \n",
    "train_labels = pd.read_csv('data/train_semeval_parids-labels.csv')[\"par_id\"]\n",
    "val_labels = pd.read_csv('data/dev_semeval_parids-labels.csv')[\"par_id\"]\n",
    "df_train = df[df[\"par_id\"].isin(train_labels)].reset_index() \n",
    "df_val = df[df[\"par_id\"].isin(val_labels)].reset_index() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df093263",
   "metadata": {},
   "source": [
    "# Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e546444c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "People who are homeless , those who were once homeless , those working with the homeless and concerned New Zealanders are being asked to share their experiences and solutions to this growing issue with the Cross-Party Homelessness Inquiry . More\n",
      "Council customers only signs would be displayed . Two of the spaces would be reserved for disabled persons and there would be five P0 spaces and eight P0 ones .\n"
     ]
    }
   ],
   "source": [
    "# Remove special characters\n",
    "SPECIAL_CHARACTERS = ['&amp;', '&lt;', '&gt;', '<h>', '\\n', '\\t']\n",
    "for char in SPECIAL_CHARACTERS:\n",
    "    df_train[\"text\"] = df_train[\"text\"].str.replace(char, \"\")\n",
    "    df_val[\"text\"] = df_val[\"text\"].str.replace(char, \"\")\n",
    "\n",
    "\n",
    "print(df_train[\"text\"].iloc[55])\n",
    "\n",
    "\n",
    "# Replace numbers with 0\n",
    "df_train[\"text\"] = df_train[\"text\"].str.replace(r\"\\d+\", \"0\", regex=True)\n",
    "df_val[\"text\"] = df_val[\"text\"].str.replace(r\"\\d+\", \"0\", regex=True)\n",
    "\n",
    "print(df_train[\"text\"].iloc[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d259ae1",
   "metadata": {},
   "source": [
    "# Oversample the minority class\n",
    "For each keyword category, inflate the number of positive examples to a certain percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b24af13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bool_labels\n",
       "False    7581\n",
       "True     2527\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "POSITIVE_PERCENTAGE = 25\n",
    "\n",
    "\n",
    "# Find all the unique keywords in the training dataset\n",
    "keywords = pd.unique(df_train[\"keyword\"])\n",
    "\n",
    "\n",
    "# Extract the sub-dataset for each keyword\n",
    "for keyword in keywords:\n",
    "    subdata = df_train[df_train[\"keyword\"] == keyword]\n",
    "    rows = subdata.shape[0]\n",
    "\n",
    "\n",
    "    # Find the number of positive entires x\n",
    "    subdata_positive = subdata[subdata[\"bool_labels\"] == True]\n",
    "    positive_rows = subdata_positive.shape[0]\n",
    "\n",
    "\n",
    "    # Calculate the number of additional samples needed to make the positive class reach the desired percentage\n",
    "    # (p+x)/(r+x) = POS PERCENTAGE\n",
    "    n_samples = round((100*positive_rows-POSITIVE_PERCENTAGE*rows)/(POSITIVE_PERCENTAGE-100)*1.0)\n",
    "\n",
    "\n",
    "    # Sample with replacement from the sub dataset and add new rows\n",
    "    sampled = subdata_positive.sample(n_samples, replace=True).reset_index(drop=True)\n",
    "   \n",
    "    # concat with the main df\n",
    "    df_train = pd.concat([df_train, sampled], ignore_index=True)\n",
    "\n",
    "\n",
    "df_train[\"bool_labels\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747d4f9f",
   "metadata": {},
   "source": [
    "# Coreference resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c62564d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/bm1325/dl_cw_1/dlvenv/lib/python3.12/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/vol/bitbucket/bm1325/dl_cw_1/dlvenv/lib/python3.12/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/vol/bitbucket/bm1325/dl_cw_1/dlvenv/lib/python3.12/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "03/01/2026 13:06:09 - INFO - \t missing_keys: []\n",
      "03/01/2026 13:06:09 - INFO - \t unexpected_keys: []\n",
      "03/01/2026 13:06:09 - INFO - \t mismatched_keys: []\n",
      "03/01/2026 13:06:09 - INFO - \t error_msgs: []\n",
      "03/01/2026 13:06:09 - INFO - \t Model Parameters: 90.5M, Transformer: 82.1M, Coref head: 8.4M\n",
      "03/01/2026 13:06:09 - INFO - \t Tokenize 3 inputs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0289f0b628194b12bc4d8e095d8da32b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/01/2026 13:06:09 - INFO - \t ***** Running Inference on 3 texts *****\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60e998bf27e544a0a3eb8684c4db7206",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are so happy to see you using We coref package . our coref package is very fast !\n",
      "He said the CEO of Apple was happy. the CEO of Apple later confirmed it .\n",
      "Dr. Lester Keith , doctor and professor of business administration , and others are checking with local transportation groups to see if local transportation groups can bring those in need of a meal to the college for the 4 p.m. dinner . Dr. Lester Keith , doctor and professor of business administration , and others will also be contacting local soup kitchens as a pickup location and will work with local soup kitchens to transport any leftovers to local soup kitchens so there is no wasted food , Dr. Lester Keith , doctor and professor of business administration said .\n"
     ]
    }
   ],
   "source": [
    "from fastcoref import FCoref\n",
    "\n",
    "# define the model once\n",
    "model = FCoref(device='cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "def coreference_resolution(model, text):\n",
    "    # Batch coreference resolution for all texts \n",
    "    preds = model.predict(\n",
    "        texts = text\n",
    "    )\n",
    "\n",
    "    # Iterate each row of the list to substitute the pronouns / references with entity names \n",
    "    result = [] \n",
    "    for i in range(len(text)): \n",
    "        sent = text[i] \n",
    "        clusters = preds[i].get_clusters(as_strings = False) \n",
    "\n",
    "        # create mappings from each pronoun indices -> entities TEXT\n",
    "        d = {}\n",
    "        for cluster in clusters:\n",
    "            entity = cluster[0]    # IMPORTANT - The first entity is assumed to be the main entity here. Use POS tagging to further improve this. \n",
    "            refs = cluster[1::]\n",
    "\n",
    "            for ref in refs: \n",
    "                d[ref] = sent[entity[0]:entity[1]]\n",
    "\n",
    "        # for each pronoun index (key), replace by their entity text (value) \n",
    "        sorted_keys = sorted(d.keys(), reverse = True) \n",
    "        for key in sorted_keys: \n",
    "            start, end = key \n",
    "            sent = sent[:start] + d[key] + sent[end:]\n",
    "\n",
    "        result.append(sent) \n",
    "\n",
    "    return result \n",
    "\n",
    "   \n",
    "\n",
    "test = coreference_resolution(model, ['We are so happy to see you using our coref package . This package is very fast !', \n",
    "                                     'He said the CEO of Apple was happy. Tim Cook later confirmed it .', \n",
    "                                     \"Dr. Lester Keith , doctor and professor of business administration , and others are checking with local transportation groups to see if they can bring those in need of a meal to the college for the 4 p.m. dinner . We will also be contacting local soup kitchens as a pickup location and will work with them to transport any leftovers to them so there is no wasted food , Dr. Keith said .\"])\n",
    "print(test[0]) \n",
    "print(test[1]) \n",
    "print(test[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b3c2ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/01/2026 13:06:09 - INFO - \t Tokenize 10108 inputs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0df30078430465eb2a0b75683a0e7de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10108 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/01/2026 13:06:25 - INFO - \t ***** Running Inference on 10108 texts *****\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca8ed068ed87443cb60a5ce59c4e5177",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/10108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/01/2026 13:06:43 - INFO - \t Tokenize 2093 inputs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "693c29975ab04adeafb4562212ab5576",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2093 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/01/2026 13:06:47 - INFO - \t ***** Running Inference on 2093 texts *****\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd02b22fcf374387a9ba89e8cc544efb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/2093 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_coref = coreference_resolution(model, df_train[\"text\"].tolist())\n",
    "df_train[\"text_cr\"] = pd.Series(train_coref) \n",
    "\n",
    "val_coref = coreference_resolution(model, df_val[\"text\"].tolist())\n",
    "df_val[\"text_cr\"] = pd.Series(val_coref) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a419d0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" The regional brands so far lag behind the global and big international Chinese handset players in 0G and they have looked vulnerable to failing to jump the generation successfully and lose their place . \"\n",
      "\" The regional brands so far lag behind the global and big international Chinese handset players in 0G and The regional brands have looked vulnerable to failing to jump the generation successfully and lose The regional brands place . \"\n",
      "BUSINESSMAN Norberto Quisumbing Jr . of the Norkis Group of Companies has a challenge for families who can spare some of what they have : why not adopt poor families and help them break the cycle of poverty ?\n",
      "BUSINESSMAN Norberto Quisumbing Jr . of the Norkis Group of Companies has a challenge for families who can spare some of what families who can spare some of what they have have : why not adopt poor families and help poor families break the cycle of poverty ?\n"
     ]
    }
   ],
   "source": [
    "print(df_train[\"text\"].iloc[23])\n",
    "print(df_train[\"text_cr\"].iloc[23])\n",
    "\n",
    "print(df_val[\"text\"].iloc[23])\n",
    "print(df_val[\"text_cr\"].iloc[23])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc36eaee",
   "metadata": {},
   "source": [
    "# Add contextual information to the text tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78ba1672",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_info(df): \n",
    "    # Append the keyword and country code to the text, and separate them with additional separator tokens\n",
    "    # Remove dashes in the keyword to match the format in the texts \n",
    "    return df[\"keyword\"].str.replace('-', \" \") + \"[SEP]\" + df[\"country_code\"] + \"[SEP]\" + df[\"text_cr\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344d82c0",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a96d21bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fad8c0a6197b474396373e23f3919ee8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3203da0054b8400bb97e75ff6fe53587",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07cc6508d2a14b38aa4bcaef205c0624",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5becd19f75524765abaae3dd40a8c924",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, AutoConfig, Trainer, TrainingArguments\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\") \n",
    "\n",
    "# Create text with contextual information \n",
    "def tokenize(df): \n",
    "    text_with_context = add_info(df) \n",
    "\n",
    "    encoding = tokenizer(\n",
    "        text_with_context.tolist(), \n",
    "        padding=\"max_length\",   # Add padding to shorter sentences \n",
    "        max_length=256,\n",
    "        truncation = True, \n",
    "        return_attention_mask = True \n",
    "    )\n",
    "\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e03929f",
   "metadata": {},
   "source": [
    "# Convert to pyTorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d1a790e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from datasets import Dataset\n",
    "\n",
    "def to_dataset(df): \n",
    "    # Obtain tokens (input_ids, attention_mask) from the dataset \n",
    "    encoding = tokenize(df) \n",
    "\n",
    "    # Return huggingface dataset \n",
    "    return Dataset.from_dict({\n",
    "        \"input_ids\": encoding[\"input_ids\"], \n",
    "        \"attention_mask\": encoding[\"attention_mask\"], \n",
    "        \"label\": df[\"label\"].values \n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0df62041",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = to_dataset(df_train)\n",
    "val_dataset = to_dataset(df_val) \n",
    "\n",
    "# set to torch format \n",
    "train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "val_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8937900",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a7798259",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Calculate metrics \n",
    "    accuracy = accuracy_score(labels, predictions) \n",
    "    precision = precision_score(labels, predictions, average='macro') \n",
    "    recall = recall_score(labels, predictions, average='macro') \n",
    "    f1 = f1_score(labels, predictions, average='macro') \n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy, \n",
    "        \"precision\": precision, \n",
    "        \"recall\": recall, \n",
    "        \"f1\": f1 \n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "22ffcf80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/bm1325/dl_cw_1/dlvenv/lib/python3.12/site-packages/accelerate/accelerator.py:439: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Load roberta sequence classification model \n",
    "config = AutoConfig.from_pretrained(\"bert-base-uncased\", num_labels=5)  # Predict labels instead of binary classification which is done later \n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", config = config)\n",
    "model.resize_token_embeddings(len(tokenizer)) \n",
    "\n",
    "# Core hyperparameters \n",
    "BATCH_SIZE = 32\n",
    "N_EPOCHS = 5 \n",
    "\n",
    "# Set up training arguments \n",
    "training_args = TrainingArguments(\n",
    "    fp16=True, \n",
    "    num_train_epochs=N_EPOCHS, \n",
    "    learning_rate=2e-5, \n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=500, \n",
    "    save_strategy=\"epoch\",  # low disk space \n",
    "    load_best_model_at_end=True, \n",
    "    metric_for_best_model='f1',\n",
    "    logging_steps=50,\n",
    "    output_dir=\"./checkpoints/bert_improved\", \n",
    "    evaluation_strategy=\"epoch\", \n",
    "    per_device_eval_batch_size=BATCH_SIZE, \n",
    "    per_device_train_batch_size=BATCH_SIZE, \n",
    ")\n",
    "\n",
    "# Set up trainer \n",
    "trainer = Trainer(\n",
    "    model = model, \n",
    "    args = training_args, \n",
    "    train_dataset=train_dataset, \n",
    "    eval_dataset=val_dataset, \n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f05ab969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7987, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.16}\n",
      "{'loss': 1.3142, 'learning_rate': 3.96e-06, 'epoch': 0.32}\n",
      "{'loss': 1.0449, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.47}\n",
      "{'loss': 0.9498, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.63}\n",
      "{'loss': 0.9078, 'learning_rate': 9.920000000000002e-06, 'epoch': 0.79}\n",
      "{'loss': 0.8397, 'learning_rate': 1.1920000000000001e-05, 'epoch': 0.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/bm1325/dl_cw_1/dlvenv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5948612093925476, 'eval_accuracy': 0.8103201146679407, 'eval_precision': 0.215835533760845, 'eval_recall': 0.2758489644843535, 'eval_f1': 0.23956264624178933, 'eval_runtime': 17.5729, 'eval_samples_per_second': 119.104, 'eval_steps_per_second': 3.756, 'epoch': 1.0}\n",
      "{'loss': 0.7876, 'learning_rate': 1.392e-05, 'epoch': 1.11}\n",
      "{'loss': 0.7259, 'learning_rate': 1.5920000000000003e-05, 'epoch': 1.27}\n",
      "{'loss': 0.7082, 'learning_rate': 1.792e-05, 'epoch': 1.42}\n",
      "{'loss': 0.6669, 'learning_rate': 1.9920000000000002e-05, 'epoch': 1.58}\n",
      "{'loss': 0.6064, 'learning_rate': 1.9111111111111113e-05, 'epoch': 1.74}\n",
      "{'loss': 0.6361, 'learning_rate': 1.8185185185185186e-05, 'epoch': 1.9}\n",
      "{'eval_loss': 0.5668193101882935, 'eval_accuracy': 0.8155757286192069, 'eval_precision': 0.2984151341843481, 'eval_recall': 0.3356793566111242, 'eval_f1': 0.3159116659424944, 'eval_runtime': 17.4032, 'eval_samples_per_second': 120.265, 'eval_steps_per_second': 3.792, 'epoch': 2.0}\n",
      "{'loss': 0.5387, 'learning_rate': 1.725925925925926e-05, 'epoch': 2.06}\n",
      "{'loss': 0.437, 'learning_rate': 1.6333333333333335e-05, 'epoch': 2.22}\n",
      "{'loss': 0.4736, 'learning_rate': 1.5407407407407408e-05, 'epoch': 2.37}\n",
      "{'loss': 0.4439, 'learning_rate': 1.4481481481481483e-05, 'epoch': 2.53}\n",
      "{'loss': 0.4081, 'learning_rate': 1.3555555555555557e-05, 'epoch': 2.69}\n",
      "{'loss': 0.3955, 'learning_rate': 1.2629629629629632e-05, 'epoch': 2.85}\n",
      "{'eval_loss': 0.6083663702011108, 'eval_accuracy': 0.8112756808408982, 'eval_precision': 0.37464300843890747, 'eval_recall': 0.32687075906480934, 'eval_f1': 0.3444923887969553, 'eval_runtime': 17.226, 'eval_samples_per_second': 121.503, 'eval_steps_per_second': 3.831, 'epoch': 3.0}\n",
      "{'loss': 0.356, 'learning_rate': 1.1703703703703703e-05, 'epoch': 3.01}\n",
      "{'loss': 0.2633, 'learning_rate': 1.0777777777777778e-05, 'epoch': 3.16}\n",
      "{'loss': 0.2735, 'learning_rate': 9.851851851851852e-06, 'epoch': 3.32}\n",
      "{'loss': 0.2767, 'learning_rate': 8.925925925925927e-06, 'epoch': 3.48}\n",
      "{'loss': 0.2063, 'learning_rate': 8.000000000000001e-06, 'epoch': 3.64}\n",
      "{'loss': 0.2349, 'learning_rate': 7.074074074074074e-06, 'epoch': 3.8}\n",
      "{'loss': 0.2381, 'learning_rate': 6.148148148148149e-06, 'epoch': 3.96}\n",
      "{'eval_loss': 0.7044034600257874, 'eval_accuracy': 0.7959866220735786, 'eval_precision': 0.40199944719759395, 'eval_recall': 0.3431487125051175, 'eval_f1': 0.36318224049375847, 'eval_runtime': 17.311, 'eval_samples_per_second': 120.906, 'eval_steps_per_second': 3.813, 'epoch': 4.0}\n",
      "{'loss': 0.1775, 'learning_rate': 5.2222222222222226e-06, 'epoch': 4.11}\n",
      "{'loss': 0.1535, 'learning_rate': 4.296296296296296e-06, 'epoch': 4.27}\n",
      "{'loss': 0.1607, 'learning_rate': 3.3703703703703705e-06, 'epoch': 4.43}\n",
      "{'loss': 0.1625, 'learning_rate': 2.4444444444444447e-06, 'epoch': 4.59}\n",
      "{'loss': 0.154, 'learning_rate': 1.5185185185185186e-06, 'epoch': 4.75}\n",
      "{'loss': 0.1471, 'learning_rate': 5.925925925925927e-07, 'epoch': 4.91}\n",
      "{'eval_loss': 0.7682408094406128, 'eval_accuracy': 0.7978977544194935, 'eval_precision': 0.40264876908068026, 'eval_recall': 0.3477541401016615, 'eval_f1': 0.36613421906193344, 'eval_runtime': 17.546, 'eval_samples_per_second': 119.286, 'eval_steps_per_second': 3.762, 'epoch': 5.0}\n",
      "{'train_runtime': 1344.6331, 'train_samples_per_second': 37.586, 'train_steps_per_second': 1.175, 'train_loss': 0.5241040136240706, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1580, training_loss=0.5241040136240706, metrics={'train_runtime': 1344.6331, 'train_samples_per_second': 37.586, 'train_steps_per_second': 1.175, 'train_loss': 0.5241040136240706, 'epoch': 5.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2fa92ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7682408094406128, 'eval_accuracy': 0.7978977544194935, 'eval_precision': 0.40264876908068026, 'eval_recall': 0.3477541401016615, 'eval_f1': 0.36613421906193344, 'eval_runtime': 16.6769, 'eval_samples_per_second': 125.503, 'eval_steps_per_second': 3.958, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.7682408094406128,\n",
       " 'eval_accuracy': 0.7978977544194935,\n",
       " 'eval_precision': 0.40264876908068026,\n",
       " 'eval_recall': 0.3477541401016615,\n",
       " 'eval_f1': 0.36613421906193344,\n",
       " 'eval_runtime': 16.6769,\n",
       " 'eval_samples_per_second': 125.503,\n",
       " 'eval_steps_per_second': 3.958,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c79bff",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "55a4f209",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('models/model_bert_improved')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dlvenv)",
   "language": "python",
   "name": "dlvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
