{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c219c3dd",
   "metadata": {},
   "source": [
    "# For colab environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07ada328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk transformers==4.35.0 torch==2.6.0 torchvision==0.21.0 datasets accelerate==0.24.0 huggingface==0.0.1 datasets==2.14.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26401123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "2.6.0+cu124\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "print(torch.cuda.is_available())\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fc2925e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/BernardMoy/NLP-PCL-Classification.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3efe8fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd NLP-PCL-Classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5eec618c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Feb 28 21:17:03 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.163.01             Driver Version: 550.163.01     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce GTX TITAN X     Off |   00000000:02:00.0  On |                  N/A |\n",
      "| 22%   44C    P8             31W /  250W |     172MiB /  12288MiB |     26%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A     48858      G   /usr/lib/xorg/Xorg                             72MiB |\n",
      "|    0   N/A  N/A    167071      G   /usr/bin/kalendarac                             1MiB |\n",
      "|    0   N/A  N/A    169421      G   /usr/share/code/code                           87MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bb33de",
   "metadata": {},
   "source": [
    "# Load train and validation data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb2a830f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from matplotlib import pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "df = pd.read_csv('data/dontpatronizeme_pcl.tsv', sep='\\t')\n",
    "\n",
    "# Remove rows with NA labels \n",
    "df = df.dropna() \n",
    "\n",
    "# Add a bool_labels column for binary classification\n",
    "df[\"bool_labels\"] = df[\"label\"] > 1   # is PCL if >1\n",
    "\n",
    "# train val split \n",
    "train_labels = pd.read_csv('data/train_semeval_parids-labels.csv')[\"par_id\"]\n",
    "val_labels = pd.read_csv('data/dev_semeval_parids-labels.csv')[\"par_id\"]\n",
    "df_train = df[df[\"par_id\"].isin(train_labels)].reset_index() \n",
    "df_val = df[df[\"par_id\"].isin(val_labels)].reset_index() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d259ae1",
   "metadata": {},
   "source": [
    "# Oversample the minority class\n",
    "For each keyword category, inflate the number of positive examples to a certain percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b24af13",
   "metadata": {},
   "outputs": [],
   "source": [
    "POSITIVE_PERCENTAGE = 25\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747d4f9f",
   "metadata": {},
   "source": [
    "# Coreference resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c62564d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/bm1325/dl_cw_1/dlvenv/lib/python3.12/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/vol/bitbucket/bm1325/dl_cw_1/dlvenv/lib/python3.12/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/vol/bitbucket/bm1325/dl_cw_1/dlvenv/lib/python3.12/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "02/28/2026 21:17:38 - INFO - \t missing_keys: []\n",
      "02/28/2026 21:17:38 - INFO - \t unexpected_keys: []\n",
      "02/28/2026 21:17:38 - INFO - \t mismatched_keys: []\n",
      "02/28/2026 21:17:38 - INFO - \t error_msgs: []\n",
      "02/28/2026 21:17:38 - INFO - \t Model Parameters: 90.5M, Transformer: 82.1M, Coref head: 8.4M\n",
      "02/28/2026 21:17:38 - INFO - \t Tokenize 3 inputs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71227fd82cf24a989f8d12043d668943",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/28/2026 21:17:39 - INFO - \t ***** Running Inference on 3 texts *****\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d5fbe0badc64025babc0e6d7491fd7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are so happy to see you using We coref package . our coref package is very fast !\n",
      "He said the CEO of Apple was happy. the CEO of Apple later confirmed it .\n",
      "Dr. Lester Keith , doctor and professor of business administration , and others are checking with local transportation groups to see if local transportation groups can bring those in need of a meal to the college for the 4 p.m. dinner . Dr. Lester Keith , doctor and professor of business administration , and others will also be contacting local soup kitchens as a pickup location and will work with local soup kitchens to transport any leftovers to local soup kitchens so there is no wasted food , Dr. Lester Keith , doctor and professor of business administration said .\n"
     ]
    }
   ],
   "source": [
    "from fastcoref import FCoref\n",
    "\n",
    "# define the model once\n",
    "model = FCoref(device='cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "def coreference_resolution(model, text):\n",
    "    # Batch coreference resolution for all texts \n",
    "    preds = model.predict(\n",
    "        texts = text\n",
    "    )\n",
    "\n",
    "    # Iterate each row of the list to substitute the pronouns / references with entity names \n",
    "    result = [] \n",
    "    for i in range(len(text)): \n",
    "        sent = text[i] \n",
    "        clusters = preds[i].get_clusters(as_strings = False) \n",
    "\n",
    "        # create mappings from each pronoun indices -> entities TEXT\n",
    "        d = {}\n",
    "        for cluster in clusters:\n",
    "            entity = cluster[0]    # IMPORTANT - The first entity is assumed to be the main entity here. Use POS tagging to further improve this. \n",
    "            refs = cluster[1::]\n",
    "\n",
    "            for ref in refs: \n",
    "                d[ref] = sent[entity[0]:entity[1]]\n",
    "\n",
    "        # for each pronoun index (key), replace by their entity text (value) \n",
    "        sorted_keys = sorted(d.keys(), reverse = True) \n",
    "        for key in sorted_keys: \n",
    "            start, end = key \n",
    "            sent = sent[:start] + d[key] + sent[end:]\n",
    "\n",
    "        result.append(sent) \n",
    "\n",
    "    return result \n",
    "\n",
    "   \n",
    "\n",
    "test = coreference_resolution(model, ['We are so happy to see you using our coref package . This package is very fast !', \n",
    "                                     'He said the CEO of Apple was happy. Tim Cook later confirmed it .', \n",
    "                                     \"Dr. Lester Keith , doctor and professor of business administration , and others are checking with local transportation groups to see if they can bring those in need of a meal to the college for the 4 p.m. dinner . We will also be contacting local soup kitchens as a pickup location and will work with them to transport any leftovers to them so there is no wasted food , Dr. Keith said .\"])\n",
    "print(test[0]) \n",
    "print(test[1]) \n",
    "print(test[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b3c2ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/28/2026 21:17:39 - INFO - \t Tokenize 8375 inputs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a64556463a4d48629d9f8db915ef41f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8375 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/28/2026 21:17:52 - INFO - \t ***** Running Inference on 8375 texts *****\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a45c1f17fcaf4a28963bbb050a7efc06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/8375 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/28/2026 21:18:12 - INFO - \t Tokenize 2093 inputs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22d8858033e34d3e923c05546092b8a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2093 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/28/2026 21:18:16 - INFO - \t ***** Running Inference on 2093 texts *****\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "480f2c3cb8e641f88c4393899a75e9bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/2093 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_coref = coreference_resolution(model, df_train[\"text\"].tolist())\n",
    "df_train[\"text_cr\"] = pd.Series(train_coref) \n",
    "\n",
    "val_coref = coreference_resolution(model, df_val[\"text\"].tolist())\n",
    "df_val[\"text_cr\"] = pd.Series(val_coref) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a419d0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" The regional brands so far lag behind the global and big international Chinese handset players in 4G and they have looked vulnerable to failing to jump the generation successfully and lose their place . \"\n",
      "\" The regional brands so far lag behind the global and big international Chinese handset players in 4G and The regional brands have looked vulnerable to failing to jump the generation successfully and lose The regional brands place . \"\n",
      "BUSINESSMAN Norberto Quisumbing Jr . of the Norkis Group of Companies has a challenge for families who can spare some of what they have : why not adopt poor families and help them break the cycle of poverty ?\n",
      "BUSINESSMAN Norberto Quisumbing Jr . of the Norkis Group of Companies has a challenge for families who can spare some of what families who can spare some of what they have have : why not adopt poor families and help poor families break the cycle of poverty ?\n"
     ]
    }
   ],
   "source": [
    "print(df_train[\"text\"].iloc[23])\n",
    "print(df_train[\"text_cr\"].iloc[23])\n",
    "\n",
    "print(df_val[\"text\"].iloc[23])\n",
    "print(df_val[\"text_cr\"].iloc[23])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc36eaee",
   "metadata": {},
   "source": [
    "# Add contextual information to the text tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78ba1672",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_info(df): \n",
    "    # Append the keyword and country code to the text, and separate them with roberta separators </s><s> \n",
    "    return df[\"keyword\"] + \"</s><s>\" + df[\"country_code\"] + \"</s><s>\" + df[\"text_cr\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344d82c0",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a96d21bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AutoConfig, Trainer, TrainingArguments\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\") \n",
    "\n",
    "# Create text with contextual information \n",
    "def tokenize(df): \n",
    "    text_with_context = add_info(df) \n",
    "\n",
    "    encoding = tokenizer(\n",
    "        text_with_context.tolist(), \n",
    "        padding=\"max_length\",   # Add padding to shorter sentences \n",
    "        max_length=256,\n",
    "        truncation = True, \n",
    "        return_attention_mask = True \n",
    "    )\n",
    "\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e03929f",
   "metadata": {},
   "source": [
    "# Convert to pyTorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d1a790e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from datasets import Dataset\n",
    "\n",
    "def to_dataset(df): \n",
    "    # Obtain tokens (input_ids, attention_mask) from the dataset \n",
    "    encoding = tokenize(df) \n",
    "\n",
    "    # Return huggingface dataset \n",
    "    return Dataset.from_dict({\n",
    "        \"input_ids\": encoding[\"input_ids\"], \n",
    "        \"attention_mask\": encoding[\"attention_mask\"], \n",
    "        \"label\": df[\"bool_labels\"].values \n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0df62041",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = to_dataset(df_train)\n",
    "val_dataset = to_dataset(df_val) \n",
    "\n",
    "# set to torch format \n",
    "train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "val_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8937900",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7798259",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Calculate metrics \n",
    "    accuracy = accuracy_score(labels, predictions) \n",
    "    precision = precision_score(labels, predictions) \n",
    "    recall = recall_score(labels, predictions) \n",
    "    f1 = f1_score(labels, predictions) \n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy, \n",
    "        \"precision\": precision, \n",
    "        \"recall\": recall, \n",
    "        \"f1\": f1 \n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22ffcf80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/bm1325/dl_cw_1/dlvenv/lib/python3.12/site-packages/accelerate/accelerator.py:439: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Load roberta sequence classification model \n",
    "config = AutoConfig.from_pretrained(\"roberta-base\", num_labels=2)  # Binary classification\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", config = config)\n",
    "\n",
    "# Core hyperparameters \n",
    "BATCH_SIZE = 32\n",
    "N_EPOCHS = 5 \n",
    "\n",
    "# Set up training arguments \n",
    "training_args = TrainingArguments(\n",
    "    fp16=True, \n",
    "    num_train_epochs=N_EPOCHS, \n",
    "    learning_rate=2e-5, \n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=500, \n",
    "    save_strategy=\"no\",  # low disk space \n",
    "    load_best_model_at_end=True, \n",
    "    logging_steps=50,\n",
    "    output_dir=\"./predictions\", \n",
    "    evaluation_strategy=\"no\", \n",
    "    per_device_eval_batch_size=BATCH_SIZE, \n",
    "    per_device_train_batch_size=BATCH_SIZE, \n",
    ")\n",
    "\n",
    "# Set up trainer \n",
    "trainer = Trainer(\n",
    "    model = model, \n",
    "    args = training_args, \n",
    "    train_dataset=train_dataset, \n",
    "    eval_dataset=val_dataset, \n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f05ab969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7168, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.19}\n",
      "{'loss': 0.4227, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.38}\n",
      "{'loss': 0.2993, 'learning_rate': 5.9600000000000005e-06, 'epoch': 0.57}\n",
      "{'loss': 0.2531, 'learning_rate': 7.960000000000002e-06, 'epoch': 0.76}\n",
      "{'loss': 0.2307, 'learning_rate': 9.960000000000001e-06, 'epoch': 0.95}\n",
      "{'loss': 0.2168, 'learning_rate': 1.196e-05, 'epoch': 1.15}\n",
      "{'loss': 0.2268, 'learning_rate': 1.396e-05, 'epoch': 1.34}\n",
      "{'loss': 0.1873, 'learning_rate': 1.5960000000000003e-05, 'epoch': 1.53}\n",
      "{'loss': 0.2218, 'learning_rate': 1.796e-05, 'epoch': 1.72}\n",
      "{'loss': 0.1972, 'learning_rate': 1.9960000000000002e-05, 'epoch': 1.91}\n",
      "{'loss': 0.1726, 'learning_rate': 1.8790123456790124e-05, 'epoch': 2.1}\n",
      "{'loss': 0.1481, 'learning_rate': 1.7580246913580247e-05, 'epoch': 2.29}\n",
      "{'loss': 0.1137, 'learning_rate': 1.6345679012345682e-05, 'epoch': 2.48}\n",
      "{'loss': 0.1891, 'learning_rate': 1.5111111111111112e-05, 'epoch': 2.67}\n",
      "{'loss': 0.1583, 'learning_rate': 1.3876543209876546e-05, 'epoch': 2.86}\n",
      "{'loss': 0.1289, 'learning_rate': 1.2641975308641976e-05, 'epoch': 3.05}\n",
      "{'loss': 0.091, 'learning_rate': 1.1407407407407409e-05, 'epoch': 3.24}\n",
      "{'loss': 0.0882, 'learning_rate': 1.017283950617284e-05, 'epoch': 3.44}\n",
      "{'loss': 0.0731, 'learning_rate': 8.938271604938272e-06, 'epoch': 3.63}\n",
      "{'loss': 0.0904, 'learning_rate': 7.703703703703704e-06, 'epoch': 3.82}\n",
      "{'loss': 0.0945, 'learning_rate': 6.4691358024691365e-06, 'epoch': 4.01}\n",
      "{'loss': 0.0374, 'learning_rate': 5.234567901234568e-06, 'epoch': 4.2}\n",
      "{'loss': 0.0516, 'learning_rate': 4.000000000000001e-06, 'epoch': 4.39}\n",
      "{'loss': 0.0352, 'learning_rate': 2.765432098765432e-06, 'epoch': 4.58}\n",
      "{'loss': 0.0415, 'learning_rate': 1.5308641975308644e-06, 'epoch': 4.77}\n",
      "{'loss': 0.0471, 'learning_rate': 2.9629629629629634e-07, 'epoch': 4.96}\n",
      "{'train_runtime': 1717.1918, 'train_samples_per_second': 24.386, 'train_steps_per_second': 0.763, 'train_loss': 0.173153081877541, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1310, training_loss=0.173153081877541, metrics={'train_runtime': 1717.1918, 'train_samples_per_second': 24.386, 'train_steps_per_second': 0.763, 'train_loss': 0.173153081877541, 'epoch': 5.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75b75bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.34337538480758667, 'eval_accuracy': 0.9211657907310081, 'eval_precision': 0.5955056179775281, 'eval_recall': 0.5326633165829145, 'eval_f1': 0.5623342175066313, 'eval_runtime': 28.8834, 'eval_samples_per_second': 72.464, 'eval_steps_per_second': 2.285, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.34337538480758667,\n",
       " 'eval_accuracy': 0.9211657907310081,\n",
       " 'eval_precision': 0.5955056179775281,\n",
       " 'eval_recall': 0.5326633165829145,\n",
       " 'eval_f1': 0.5623342175066313,\n",
       " 'eval_runtime': 28.8834,\n",
       " 'eval_samples_per_second': 72.464,\n",
       " 'eval_steps_per_second': 2.285,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dlvenv)",
   "language": "python",
   "name": "dlvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
