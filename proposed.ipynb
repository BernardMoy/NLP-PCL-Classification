{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c219c3dd",
   "metadata": {},
   "source": [
    "# For colab environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ada328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk transformers==4.35.0 torch==2.6.0 torchvision==0.21.0 datasets accelerate==0.24.0 huggingface==0.0.1 datasets==2.14.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26401123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "2.6.0+cu124\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "print(torch.cuda.is_available())\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc2925e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/BernardMoy/NLP-PCL-Classification.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efe8fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd NLP-PCL-Classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eec618c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bb33de",
   "metadata": {},
   "source": [
    "# Load train and validation data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb2a830f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from matplotlib import pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "df = pd.read_csv('data/dontpatronizeme_pcl.tsv', sep='\\t')\n",
    "\n",
    "# Remove rows with NA labels \n",
    "df = df.dropna() \n",
    "\n",
    "# Add a bool_labels column for binary classification\n",
    "df[\"bool_labels\"] = df[\"label\"] > 1   # is PCL if >1\n",
    "\n",
    "# train val split \n",
    "train_labels = pd.read_csv('data/train_semeval_parids-labels.csv')[\"par_id\"]\n",
    "val_labels = pd.read_csv('data/dev_semeval_parids-labels.csv')[\"par_id\"]\n",
    "df_train = df[df[\"par_id\"].isin(train_labels)].reset_index() \n",
    "df_val = df[df[\"par_id\"].isin(val_labels)].reset_index() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747d4f9f",
   "metadata": {},
   "source": [
    "# Coreference resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c62564d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/bm1325/dl_cw_1/dlvenv/lib/python3.12/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/vol/bitbucket/bm1325/dl_cw_1/dlvenv/lib/python3.12/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/vol/bitbucket/bm1325/dl_cw_1/dlvenv/lib/python3.12/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "02/28/2026 20:32:49 - INFO - \t missing_keys: []\n",
      "02/28/2026 20:32:49 - INFO - \t unexpected_keys: []\n",
      "02/28/2026 20:32:49 - INFO - \t mismatched_keys: []\n",
      "02/28/2026 20:32:49 - INFO - \t error_msgs: []\n",
      "02/28/2026 20:32:49 - INFO - \t Model Parameters: 90.5M, Transformer: 82.1M, Coref head: 8.4M\n",
      "02/28/2026 20:32:49 - INFO - \t Tokenize 3 inputs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e0d9a4af3474cce9d44a45f0ceb1bb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/28/2026 20:32:49 - INFO - \t ***** Running Inference on 3 texts *****\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf794f1a9d50461dadf4797be7e57589",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We are so happy to see you using We coref package . our coref package is very fast !', 'He said the CEO of Apple was happy. the CEO of Apple later confirmed it .', 'Dr. Lester Keith , doctor and professor of business administration , and others are checking with local transportation groups to see if local transportation groups can bring those in need of a meal to the college for the 4 p.m. dinner . Dr. Lester Keith , doctor and professor of business administration , and others will also be contacting local soup kitchens as a pickup location and will work with local soup kitchens to transport any leftovers to local soup kitchens so there is no wasted food , Dr. Lester Keith , doctor and professor of business administration said .']\n"
     ]
    }
   ],
   "source": [
    "from fastcoref import FCoref\n",
    "\n",
    "# define the model once\n",
    "model = FCoref(device='cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "def coreference_resolution(model, text):\n",
    "    # Batch coreference resolution for all texts \n",
    "    preds = model.predict(\n",
    "        texts = text\n",
    "    )\n",
    "\n",
    "    # Iterate each row of the list to substitute the pronouns / references with entity names \n",
    "    result = [] \n",
    "    for i in range(len(text)): \n",
    "        sent = text[i] \n",
    "        clusters = preds[i].get_clusters(as_strings = False) \n",
    "\n",
    "        # create mappings from each pronoun indices -> entities TEXT\n",
    "        d = {}\n",
    "        for cluster in clusters:\n",
    "            entity = cluster[0]    # IMPORTANT - The first entity is assumed to be the main entity here. Use POS tagging to further improve this. \n",
    "            refs = cluster[1::]\n",
    "\n",
    "            for ref in refs: \n",
    "                d[ref] = sent[entity[0]:entity[1]]\n",
    "\n",
    "        # for each pronoun index (key), replace by their entity text (value) \n",
    "        sorted_keys = sorted(d.keys(), reverse = True) \n",
    "        for key in sorted_keys: \n",
    "            start, end = key \n",
    "            sent = sent[:start] + d[key] + sent[end:]\n",
    "\n",
    "        result.append(sent) \n",
    "\n",
    "    return result \n",
    "\n",
    "   \n",
    "\n",
    "test = coreference_resolution(model, ['We are so happy to see you using our coref package . This package is very fast !', \n",
    "                                     'He said the CEO of Apple was happy. Tim Cook later confirmed it .', \n",
    "                                     \"Dr. Lester Keith , doctor and professor of business administration , and others are checking with local transportation groups to see if they can bring those in need of a meal to the college for the 4 p.m. dinner . We will also be contacting local soup kitchens as a pickup location and will work with them to transport any leftovers to them so there is no wasted food , Dr. Keith said .\"])\n",
    "print(test[0]) \n",
    "print(test[1]) \n",
    "print(test[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b3c2ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/28/2026 20:32:55 - INFO - \t Tokenize 8375 inputs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4acc8eb8fcba41c585a17655253ae6e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8375 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/28/2026 20:33:18 - INFO - \t ***** Running Inference on 8375 texts *****\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83504a794a334957ae641c563847c1c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/8375 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/28/2026 20:44:42 - INFO - \t Tokenize 2093 inputs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4d62397692f4fb994b6e54e4a07d3df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2093 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/28/2026 20:44:49 - INFO - \t ***** Running Inference on 2093 texts *****\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5261e6f5b91461bacdaa2db930c63de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/2093 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_coref = coreference_resolution(model, df_train[\"text\"].tolist())\n",
    "df_train[\"text_cr\"] = pd.Series(train_coref) \n",
    "\n",
    "val_coref = coreference_resolution(model, df_val[\"text\"].tolist())\n",
    "df_val[\"text_cr\"] = pd.Series(val_coref) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a419d0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" The regional brands so far lag behind the global and big international Chinese handset players in 4G and they have looked vulnerable to failing to jump the generation successfully and lose their place . \"\n",
      "\" The regional brands so far lag behind the global and big international Chinese handset players in 4G and The regional brands have looked vulnerable to failing to jump the generation successfully and lose The regional brands place . \"\n",
      "BUSINESSMAN Norberto Quisumbing Jr . of the Norkis Group of Companies has a challenge for families who can spare some of what they have : why not adopt poor families and help them break the cycle of poverty ?\n",
      "BUSINESSMAN Norberto Quisumbing Jr . of the Norkis Group of Companies has a challenge for families who can spare some of what families who can spare some of what they have have : why not adopt poor families and help poor families break the cycle of poverty ?\n"
     ]
    }
   ],
   "source": [
    "print(df_train[\"text\"].iloc[23])\n",
    "print(df_train[\"text_cr\"].iloc[23])\n",
    "\n",
    "print(df_val[\"text\"].iloc[23])\n",
    "print(df_val[\"text_cr\"].iloc[23])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc36eaee",
   "metadata": {},
   "source": [
    "# Add contextual information to the text tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78ba1672",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_info(df): \n",
    "    # Append the keyword and country code to the text, and separate them with roberta separators </s><s> \n",
    "    return df[\"keyword\"] + \"</s><s>\" + df[\"country_code\"] + \"</s><s>\" + df[\"text_cr\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344d82c0",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a96d21bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AutoConfig, Trainer, TrainingArguments\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\") \n",
    "\n",
    "# Create text with contextual information \n",
    "def tokenize(df): \n",
    "    text_with_context = add_info(df) \n",
    "\n",
    "    encoding = tokenizer(\n",
    "        text_with_context.tolist(), \n",
    "        padding=\"max_length\",   # Add padding to shorter sentences \n",
    "        max_length=256,\n",
    "        truncation = True, \n",
    "        return_attention_mask = True \n",
    "    )\n",
    "\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e03929f",
   "metadata": {},
   "source": [
    "# Convert to pyTorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d1a790e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from datasets import Dataset\n",
    "\n",
    "def to_dataset(df): \n",
    "    # Obtain tokens (input_ids, attention_mask) from the dataset \n",
    "    encoding = tokenize(df) \n",
    "\n",
    "    # Return huggingface dataset \n",
    "    return Dataset.from_dict({\n",
    "        \"input_ids\": encoding[\"input_ids\"], \n",
    "        \"attention_mask\": encoding[\"attention_mask\"], \n",
    "        \"label\": df[\"bool_labels\"].values \n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0df62041",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = to_dataset(df_train)\n",
    "val_dataset = to_dataset(df_val) \n",
    "\n",
    "# set to torch format \n",
    "train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "val_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8937900",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7798259",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Calculate metrics \n",
    "    accuracy = accuracy_score(labels, predictions) \n",
    "    precision = precision_score(labels, predictions) \n",
    "    recall = recall_score(labels, predictions) \n",
    "    f1 = f1_score(labels, predictions) \n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy, \n",
    "        \"precision\": precision, \n",
    "        \"recall\": recall, \n",
    "        \"f1\": f1 \n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22ffcf80",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "FP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation (`--fp16_full_eval`) can only be used on CUDA or NPU devices or certain XPU devices (with IPEX).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m BATCH_SIZE = \u001b[32m32\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Set up training arguments \u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m training_args = \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfp16\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2e-5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mno\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# low disk space \u001b[39;49;00m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_best_model_at_end\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./predictions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevaluation_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mno\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Set up trainer \u001b[39;00m\n\u001b[32m     24\u001b[39m trainer = Trainer(\n\u001b[32m     25\u001b[39m     model = model, \n\u001b[32m     26\u001b[39m     args = training_args, \n\u001b[32m   (...)\u001b[39m\u001b[32m     29\u001b[39m     compute_metrics=compute_metrics\n\u001b[32m     30\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:117\u001b[39m, in \u001b[36m__init__\u001b[39m\u001b[34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, evaluation_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, dispatch_batches, split_batches, include_tokens_per_second, neftune_noise_alpha)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/vol/bitbucket/bm1325/dl_cw_1/dlvenv/lib/python3.12/site-packages/transformers/training_args.py:1448\u001b[39m, in \u001b[36mTrainingArguments.__post_init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1437\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m--optim adamw_torch_fused with --fp16 requires PyTorch>2.0\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1439\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1440\u001b[39m     \u001b[38;5;28mself\u001b[39m.framework == \u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1441\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m is_torch_available()\n\u001b[32m   (...)\u001b[39m\u001b[32m   1446\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.fp16 \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fp16_full_eval)\n\u001b[32m   1447\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1448\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1449\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1450\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m (`--fp16_full_eval`) can only be used on CUDA or NPU devices or certain XPU devices (with IPEX).\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1451\u001b[39m     )\n\u001b[32m   1453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1454\u001b[39m     \u001b[38;5;28mself\u001b[39m.framework == \u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1455\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m is_torch_available()\n\u001b[32m   (...)\u001b[39m\u001b[32m   1462\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.bf16 \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bf16_full_eval)\n\u001b[32m   1463\u001b[39m ):\n\u001b[32m   1464\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1465\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mBF16 Mixed precision training with AMP (`--bf16`) and BF16 half precision evaluation\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1466\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m (`--bf16_full_eval`) can only be used on CUDA, XPU (with IPEX), NPU or CPU/TPU/NeuronCore devices.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1467\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: FP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation (`--fp16_full_eval`) can only be used on CUDA or NPU devices or certain XPU devices (with IPEX)."
     ]
    }
   ],
   "source": [
    "# Load roberta sequence classification model \n",
    "config = AutoConfig.from_pretrained(\"roberta-base\", num_labels=2)  # Binary classification\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", config = config)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Set up training arguments \n",
    "training_args = TrainingArguments(\n",
    "    fp16=True, \n",
    "    num_train_epochs=5, \n",
    "    learning_rate=2e-5, \n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=500, \n",
    "    save_strategy=\"no\",  # low disk space \n",
    "    load_best_model_at_end=True, \n",
    "    logging_steps=50,\n",
    "    output_dir=\"./predictions\", \n",
    "    evaluation_strategy=\"no\", \n",
    "    per_device_eval_batch_size=BATCH_SIZE, \n",
    "    per_device_train_batch_size=BATCH_SIZE, \n",
    ")\n",
    "\n",
    "# Set up trainer \n",
    "trainer = Trainer(\n",
    "    model = model, \n",
    "    args = training_args, \n",
    "    train_dataset=train_dataset, \n",
    "    eval_dataset=val_dataset, \n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05ab969",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b75bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dlvenv)",
   "language": "python",
   "name": "dlvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
