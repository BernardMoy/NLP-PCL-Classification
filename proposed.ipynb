{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c219c3dd",
   "metadata": {},
   "source": [
    "# For colab environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07ada328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk transformers==4.35.0 torch==2.6.0 torchvision==0.21.0 datasets accelerate==0.24.0 huggingface==0.0.1 datasets==2.14.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26401123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "2.6.0+cu124\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "print(torch.cuda.is_available())\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fc2925e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/BernardMoy/NLP-PCL-Classification.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3efe8fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd NLP-PCL-Classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5eec618c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Feb 28 21:55:04 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.163.01             Driver Version: 550.163.01     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce GTX TITAN X     Off |   00000000:02:00.0  On |                  N/A |\n",
      "| 22%   60C    P0             89W /  250W |     440MiB /  12288MiB |      3%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A     48858      G   /usr/lib/xorg/Xorg                            122MiB |\n",
      "|    0   N/A  N/A    167071      G   /usr/bin/kalendarac                             1MiB |\n",
      "|    0   N/A  N/A    169421      G   /usr/share/code/code                          140MiB |\n",
      "|    0   N/A  N/A    184040      G   /usr/lib/firefox/firefox                      164MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bb33de",
   "metadata": {},
   "source": [
    "# Load train and validation data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb2a830f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from matplotlib import pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "df = pd.read_csv('data/dontpatronizeme_pcl.tsv', sep='\\t')\n",
    "\n",
    "# Remove rows with NA labels \n",
    "df = df.dropna() \n",
    "\n",
    "# Add a bool_labels column for binary classification\n",
    "df[\"bool_labels\"] = df[\"label\"] > 1   # is PCL if >1\n",
    "\n",
    "# train val split \n",
    "train_labels = pd.read_csv('data/train_semeval_parids-labels.csv')[\"par_id\"]\n",
    "val_labels = pd.read_csv('data/dev_semeval_parids-labels.csv')[\"par_id\"]\n",
    "df_train = df[df[\"par_id\"].isin(train_labels)].reset_index() \n",
    "df_val = df[df[\"par_id\"].isin(val_labels)].reset_index() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df093263",
   "metadata": {},
   "source": [
    "# Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e546444c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove special characters\n",
    "SPECIAL_CHARACTERS = ['&amp;', '&lt;', '&gt;', '<h>', '\\n', '\\t']\n",
    "for char in SPECIAL_CHARACTERS:\n",
    "    df_train[\"text\"] = df_train[\"text\"].str.replace(char, \"\")\n",
    "    df_val[\"text\"] = df_val[\"text\"].str.replace(char, \"\")\n",
    "\n",
    "\n",
    "print(df_train[\"text\"].iloc[55])\n",
    "\n",
    "\n",
    "# Replace numbers with <NUM>\n",
    "df_train[\"text\"] = df_train[\"text\"].str.replace(r\"\\d+\", \"<NUM>\", regex=True)\n",
    " \n",
    "print(df_train[\"text\"].iloc[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d259ae1",
   "metadata": {},
   "source": [
    "# Oversample the minority class\n",
    "For each keyword category, inflate the number of positive examples to a certain percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b24af13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bool_labels\n",
       "False    7581\n",
       "True     2527\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "POSITIVE_PERCENTAGE = 25\n",
    "\n",
    "\n",
    "# Find all the unique keywords in the training dataset\n",
    "keywords = pd.unique(df_train[\"keyword\"])\n",
    "\n",
    "\n",
    "# Extract the sub-dataset for each keyword\n",
    "for keyword in keywords:\n",
    "    subdata = df_train[df_train[\"keyword\"] == keyword]\n",
    "    rows = subdata.shape[0]\n",
    "\n",
    "\n",
    "    # Find the number of positive entires x\n",
    "    subdata_positive = subdata[subdata[\"bool_labels\"] == True]\n",
    "    positive_rows = subdata_positive.shape[0]\n",
    "\n",
    "\n",
    "    # Calculate the number of additional samples needed to make the positive class reach the desired percentage\n",
    "    # (p+x)/(r+x) = POS PERCENTAGE\n",
    "    n_samples = round((100*positive_rows-POSITIVE_PERCENTAGE*rows)/(POSITIVE_PERCENTAGE-100)*1.0)\n",
    "\n",
    "\n",
    "    # Sample with replacement from the sub dataset and add new rows\n",
    "    sampled = subdata_positive.sample(n_samples, replace=True).reset_index(drop=True)\n",
    "   \n",
    "    # concat with the main df\n",
    "    df_train = pd.concat([df_train, sampled], ignore_index=True)\n",
    "\n",
    "\n",
    "df_train[\"bool_labels\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747d4f9f",
   "metadata": {},
   "source": [
    "# Coreference resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c62564d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/bm1325/dl_cw_1/dlvenv/lib/python3.12/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/vol/bitbucket/bm1325/dl_cw_1/dlvenv/lib/python3.12/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/vol/bitbucket/bm1325/dl_cw_1/dlvenv/lib/python3.12/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "02/28/2026 21:55:31 - INFO - \t missing_keys: []\n",
      "02/28/2026 21:55:31 - INFO - \t unexpected_keys: []\n",
      "02/28/2026 21:55:31 - INFO - \t mismatched_keys: []\n",
      "02/28/2026 21:55:31 - INFO - \t error_msgs: []\n",
      "02/28/2026 21:55:31 - INFO - \t Model Parameters: 90.5M, Transformer: 82.1M, Coref head: 8.4M\n",
      "02/28/2026 21:55:31 - INFO - \t Tokenize 3 inputs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d5c47b9ce864069b0f029d8b3763f5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/28/2026 21:55:31 - INFO - \t ***** Running Inference on 3 texts *****\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c359710c8104df29e17c17f01389d76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are so happy to see you using We coref package . our coref package is very fast !\n",
      "He said the CEO of Apple was happy. the CEO of Apple later confirmed it .\n",
      "Dr. Lester Keith , doctor and professor of business administration , and others are checking with local transportation groups to see if local transportation groups can bring those in need of a meal to the college for the 4 p.m. dinner . Dr. Lester Keith , doctor and professor of business administration , and others will also be contacting local soup kitchens as a pickup location and will work with local soup kitchens to transport any leftovers to local soup kitchens so there is no wasted food , Dr. Lester Keith , doctor and professor of business administration said .\n"
     ]
    }
   ],
   "source": [
    "from fastcoref import FCoref\n",
    "\n",
    "# define the model once\n",
    "model = FCoref(device='cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "def coreference_resolution(model, text):\n",
    "    # Batch coreference resolution for all texts \n",
    "    preds = model.predict(\n",
    "        texts = text\n",
    "    )\n",
    "\n",
    "    # Iterate each row of the list to substitute the pronouns / references with entity names \n",
    "    result = [] \n",
    "    for i in range(len(text)): \n",
    "        sent = text[i] \n",
    "        clusters = preds[i].get_clusters(as_strings = False) \n",
    "\n",
    "        # create mappings from each pronoun indices -> entities TEXT\n",
    "        d = {}\n",
    "        for cluster in clusters:\n",
    "            entity = cluster[0]    # IMPORTANT - The first entity is assumed to be the main entity here. Use POS tagging to further improve this. \n",
    "            refs = cluster[1::]\n",
    "\n",
    "            for ref in refs: \n",
    "                d[ref] = sent[entity[0]:entity[1]]\n",
    "\n",
    "        # for each pronoun index (key), replace by their entity text (value) \n",
    "        sorted_keys = sorted(d.keys(), reverse = True) \n",
    "        for key in sorted_keys: \n",
    "            start, end = key \n",
    "            sent = sent[:start] + d[key] + sent[end:]\n",
    "\n",
    "        result.append(sent) \n",
    "\n",
    "    return result \n",
    "\n",
    "   \n",
    "\n",
    "test = coreference_resolution(model, ['We are so happy to see you using our coref package . This package is very fast !', \n",
    "                                     'He said the CEO of Apple was happy. Tim Cook later confirmed it .', \n",
    "                                     \"Dr. Lester Keith , doctor and professor of business administration , and others are checking with local transportation groups to see if they can bring those in need of a meal to the college for the 4 p.m. dinner . We will also be contacting local soup kitchens as a pickup location and will work with them to transport any leftovers to them so there is no wasted food , Dr. Keith said .\"])\n",
    "print(test[0]) \n",
    "print(test[1]) \n",
    "print(test[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b3c2ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/28/2026 21:55:31 - INFO - \t Tokenize 10108 inputs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5d76408cf594dc886f03959f42504d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10108 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/28/2026 21:55:47 - INFO - \t ***** Running Inference on 10108 texts *****\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93a03b3d48264d1cb8f363c4cd83c5e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/10108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/28/2026 21:56:12 - INFO - \t Tokenize 2093 inputs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75a16e620ce442628bd3ef4c1f3e5d02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2093 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/28/2026 21:56:15 - INFO - \t ***** Running Inference on 2093 texts *****\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac09b6eb07a84341aebdc482ce406598",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/2093 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_coref = coreference_resolution(model, df_train[\"text\"].tolist())\n",
    "df_train[\"text_cr\"] = pd.Series(train_coref) \n",
    "\n",
    "val_coref = coreference_resolution(model, df_val[\"text\"].tolist())\n",
    "df_val[\"text_cr\"] = pd.Series(val_coref) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a419d0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" The regional brands so far lag behind the global and big international Chinese handset players in 4G and they have looked vulnerable to failing to jump the generation successfully and lose their place . \"\n",
      "\" The regional brands so far lag behind the global and big international Chinese handset players in 4G and The regional brands have looked vulnerable to failing to jump the generation successfully and lose The regional brands place . \"\n",
      "BUSINESSMAN Norberto Quisumbing Jr . of the Norkis Group of Companies has a challenge for families who can spare some of what they have : why not adopt poor families and help them break the cycle of poverty ?\n",
      "BUSINESSMAN Norberto Quisumbing Jr . of the Norkis Group of Companies has a challenge for families who can spare some of what families who can spare some of what they have have : why not adopt poor families and help poor families break the cycle of poverty ?\n"
     ]
    }
   ],
   "source": [
    "print(df_train[\"text\"].iloc[23])\n",
    "print(df_train[\"text_cr\"].iloc[23])\n",
    "\n",
    "print(df_val[\"text\"].iloc[23])\n",
    "print(df_val[\"text_cr\"].iloc[23])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc36eaee",
   "metadata": {},
   "source": [
    "# Add contextual information to the text tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ba1672",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_info(df): \n",
    "    # Append the keyword and country code to the text, and separate them with roberta separators </s><s> \n",
    "    # Remove dashes in the keyword to match the format in the texts \n",
    "    return df[\"keyword\"].str.replace('-', \" \") + \"</s><s>\" + df[\"country_code\"] + \"</s><s>\" + df[\"text_cr\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344d82c0",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a96d21bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AutoConfig, Trainer, TrainingArguments\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\") \n",
    "\n",
    "# Create text with contextual information \n",
    "def tokenize(df): \n",
    "    text_with_context = add_info(df) \n",
    "\n",
    "    encoding = tokenizer(\n",
    "        text_with_context.tolist(), \n",
    "        padding=\"max_length\",   # Add padding to shorter sentences \n",
    "        max_length=256,\n",
    "        truncation = True, \n",
    "        return_attention_mask = True \n",
    "    )\n",
    "\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e03929f",
   "metadata": {},
   "source": [
    "# Convert to pyTorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d1a790e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from datasets import Dataset\n",
    "\n",
    "def to_dataset(df): \n",
    "    # Obtain tokens (input_ids, attention_mask) from the dataset \n",
    "    encoding = tokenize(df) \n",
    "\n",
    "    # Return huggingface dataset \n",
    "    return Dataset.from_dict({\n",
    "        \"input_ids\": encoding[\"input_ids\"], \n",
    "        \"attention_mask\": encoding[\"attention_mask\"], \n",
    "        \"label\": df[\"bool_labels\"].values \n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0df62041",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = to_dataset(df_train)\n",
    "val_dataset = to_dataset(df_val) \n",
    "\n",
    "# set to torch format \n",
    "train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "val_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8937900",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7798259",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Calculate metrics \n",
    "    accuracy = accuracy_score(labels, predictions) \n",
    "    precision = precision_score(labels, predictions) \n",
    "    recall = recall_score(labels, predictions) \n",
    "    f1 = f1_score(labels, predictions) \n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy, \n",
    "        \"precision\": precision, \n",
    "        \"recall\": recall, \n",
    "        \"f1\": f1 \n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22ffcf80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/bm1325/dl_cw_1/dlvenv/lib/python3.12/site-packages/accelerate/accelerator.py:439: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Load roberta sequence classification model \n",
    "config = AutoConfig.from_pretrained(\"roberta-base\", num_labels=2)  # Binary classification\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", config = config)\n",
    "\n",
    "# Core hyperparameters \n",
    "BATCH_SIZE = 32\n",
    "N_EPOCHS = 5 \n",
    "\n",
    "# Set up training arguments \n",
    "training_args = TrainingArguments(\n",
    "    fp16=True, \n",
    "    num_train_epochs=N_EPOCHS, \n",
    "    learning_rate=2e-5, \n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=500, \n",
    "    save_strategy=\"no\",  # low disk space \n",
    "    load_best_model_at_end=True, \n",
    "    logging_steps=50,\n",
    "    output_dir=\"./predictions\", \n",
    "    evaluation_strategy=\"no\", \n",
    "    per_device_eval_batch_size=BATCH_SIZE, \n",
    "    per_device_train_batch_size=BATCH_SIZE, \n",
    ")\n",
    "\n",
    "# Set up trainer \n",
    "trainer = Trainer(\n",
    "    model = model, \n",
    "    args = training_args, \n",
    "    train_dataset=train_dataset, \n",
    "    eval_dataset=val_dataset, \n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f05ab969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7131, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.16}\n",
      "{'loss': 0.5952, 'learning_rate': 3.920000000000001e-06, 'epoch': 0.32}\n",
      "{'loss': 0.5382, 'learning_rate': 5.92e-06, 'epoch': 0.47}\n",
      "{'loss': 0.4263, 'learning_rate': 7.92e-06, 'epoch': 0.63}\n",
      "{'loss': 0.3647, 'learning_rate': 9.88e-06, 'epoch': 0.79}\n",
      "{'loss': 0.3355, 'learning_rate': 1.188e-05, 'epoch': 0.95}\n",
      "{'loss': 0.2716, 'learning_rate': 1.3880000000000001e-05, 'epoch': 1.11}\n",
      "{'loss': 0.2521, 'learning_rate': 1.588e-05, 'epoch': 1.27}\n",
      "{'loss': 0.2077, 'learning_rate': 1.788e-05, 'epoch': 1.42}\n",
      "{'loss': 0.2134, 'learning_rate': 1.9880000000000003e-05, 'epoch': 1.58}\n",
      "{'loss': 0.2128, 'learning_rate': 1.9129629629629632e-05, 'epoch': 1.74}\n",
      "{'loss': 0.1957, 'learning_rate': 1.8203703703703705e-05, 'epoch': 1.9}\n",
      "{'loss': 0.1528, 'learning_rate': 1.727777777777778e-05, 'epoch': 2.06}\n",
      "{'loss': 0.0833, 'learning_rate': 1.635185185185185e-05, 'epoch': 2.22}\n",
      "{'loss': 0.0948, 'learning_rate': 1.5425925925925927e-05, 'epoch': 2.37}\n",
      "{'loss': 0.1571, 'learning_rate': 1.45e-05, 'epoch': 2.53}\n",
      "{'loss': 0.123, 'learning_rate': 1.3574074074074075e-05, 'epoch': 2.69}\n",
      "{'loss': 0.1196, 'learning_rate': 1.264814814814815e-05, 'epoch': 2.85}\n",
      "{'loss': 0.0912, 'learning_rate': 1.1722222222222224e-05, 'epoch': 3.01}\n",
      "{'loss': 0.0424, 'learning_rate': 1.0796296296296298e-05, 'epoch': 3.16}\n",
      "{'loss': 0.0584, 'learning_rate': 9.870370370370371e-06, 'epoch': 3.32}\n",
      "{'loss': 0.0634, 'learning_rate': 8.944444444444446e-06, 'epoch': 3.48}\n",
      "{'loss': 0.0482, 'learning_rate': 8.018518518518519e-06, 'epoch': 3.64}\n",
      "{'loss': 0.0372, 'learning_rate': 7.0925925925925935e-06, 'epoch': 3.8}\n",
      "{'loss': 0.0614, 'learning_rate': 6.166666666666667e-06, 'epoch': 3.96}\n",
      "{'loss': 0.0324, 'learning_rate': 5.240740740740741e-06, 'epoch': 4.11}\n",
      "{'loss': 0.0213, 'learning_rate': 4.314814814814815e-06, 'epoch': 4.27}\n",
      "{'loss': 0.029, 'learning_rate': 3.3888888888888893e-06, 'epoch': 4.43}\n",
      "{'loss': 0.0123, 'learning_rate': 2.462962962962963e-06, 'epoch': 4.59}\n",
      "{'loss': 0.0183, 'learning_rate': 1.5370370370370372e-06, 'epoch': 4.75}\n",
      "{'loss': 0.0126, 'learning_rate': 6.111111111111112e-07, 'epoch': 4.91}\n",
      "{'train_runtime': 2075.3441, 'train_samples_per_second': 24.353, 'train_steps_per_second': 0.761, 'train_loss': 0.17695390433073044, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1580, training_loss=0.17695390433073044, metrics={'train_runtime': 2075.3441, 'train_samples_per_second': 24.353, 'train_steps_per_second': 0.761, 'train_loss': 0.17695390433073044, 'epoch': 5.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75b75bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4609193801879883, 'eval_accuracy': 0.9254658385093167, 'eval_precision': 0.6462585034013606, 'eval_recall': 0.47738693467336685, 'eval_f1': 0.5491329479768786, 'eval_runtime': 29.0524, 'eval_samples_per_second': 72.042, 'eval_steps_per_second': 2.272, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.4609193801879883,\n",
       " 'eval_accuracy': 0.9254658385093167,\n",
       " 'eval_precision': 0.6462585034013606,\n",
       " 'eval_recall': 0.47738693467336685,\n",
       " 'eval_f1': 0.5491329479768786,\n",
       " 'eval_runtime': 29.0524,\n",
       " 'eval_samples_per_second': 72.042,\n",
       " 'eval_steps_per_second': 2.272,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dlvenv)",
   "language": "python",
   "name": "dlvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
